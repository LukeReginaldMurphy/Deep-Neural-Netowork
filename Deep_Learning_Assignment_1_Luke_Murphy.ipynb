{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Assignment 1: implementing a Neural Network\n",
    "\n",
    "\n",
    "Part 1: Implement Logistic Regression\n",
    "\n",
    "The primary 'fit' method (inspired from Scikit learns similar method) takes as input a list of input values 'pred_vars' of arbitrary number of feature values (so long as it is consistent) and the corresponding target variable list of binary values.\n",
    "The other paramaters are considered hyper parameters, all with a default value considered a 'good default value' by many sources.\n",
    "\n",
    "The logistic regressor then operates as a normal one would from closely following the lecture notes, using Stochastic Gradient Descent in minimising the cost function.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1616025420500,
     "user": {
      "displayName": "Reggie Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggnd3P6tdKw4jehzlA9vIGoDnCp2KJPBxWEWWmn1Q=s64",
      "userId": "00203112220589711987"
     },
     "user_tz": 0
    },
    "id": "XprdRnCP3X5y",
    "outputId": "061d3bce-e226-4822-f05e-30a890c8c1c3"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np     #For basic matrix and list operations\n",
    "import random as rand  #For random number generation\n",
    "seed = 999\n",
    "rand.seed(seed)        #So results are reproducible for comparisons\n",
    "import pandas as pd    #For loading csv files and having clean data frames\n",
    "\n",
    "\n",
    "class Logistic:\n",
    "    \n",
    "    #Primary funtion which learns the weights and biases for prediction\n",
    "    def fit(self, pred_vars,target_var,learning_rate = 0.1 ,max_iter = 10000 ,threshold = 1e-6):\n",
    "\n",
    "        #Let x and y be the predictor and target variables respectively\n",
    "        self.x = pred_vars        \n",
    "        self.y = target_var \n",
    "        self.num_vars = len(pred_vars[0])    #store number of predictor variables\n",
    "        self.num_points = len(target_var)    #store number of data points supplied\n",
    "\n",
    "        #randomly initialise w and b to float in  range [0,1]\n",
    "        self.w = [rand.random() for i in range(self.num_vars)]\n",
    "        self.b = rand.random()\n",
    "\n",
    "\n",
    "        iters = 0   #To store current iteration of regressor in order to check if max_iterations has been reached\n",
    "        j_prev = 0  #To store the previous loss value to check against current\n",
    "                    #loss value if we have reached a sufficient minima (relating to threshold)\n",
    "\n",
    "        while iters < max_iter:   #if max_iters reached, stop learning\n",
    "            rand_num = rand.sample(range(self.num_points),1)[0]  #take 1 random number in range [0,num_points-1] as python indexes starting with 0\n",
    "            #Extract this random random point from the data \n",
    "            x_i = self.x[rand_num] \n",
    "            y_i = self.y[rand_num]\n",
    "\n",
    "            y_hat = self.sigmoid(np.dot(self.w,x_i) + self.b) #representing probability of point x having class 1\n",
    "            j_curr = -(y_i*np.log(y_hat) + (1-y_i)*np.log(1-y_hat)) #objective function we wish to minimise \n",
    "\n",
    "            if abs(j_curr - j_prev) < threshold: #compare j value to previous one. if a given threshold has been reached , the algorithm has 'Learned enough'\n",
    "                #print('Threshold reached after ' + str(iters) + ' iterations')\n",
    "                break  #Stop learning\n",
    "\n",
    "            #Gradient Descent stage\n",
    "            #Adjust our previous values of w and b subject to our guess for what the target variable was and what it actually is\n",
    "            delta_w = [(y_hat - y_i)*x_i[j] for j in range(self.num_vars)]\n",
    "            delta_b = y_hat - y_i\n",
    "\n",
    "            #Update the values using the specified learning rate\n",
    "            self.w = [self.w[j] - learning_rate*delta_w[j] for j in range(self.num_vars)]\n",
    "            self.b -= learning_rate*delta_b\n",
    "\n",
    "            #Learning iteration complete..\n",
    "            iters += 1    #increment iteration at end\n",
    "            j_prev = j_curr  #update previous loss value to current one\n",
    "\n",
    "\n",
    "\n",
    "        return None #Stores values listed but doesn't explicitely return a value\n",
    "\n",
    "    def sigmoid(self,z): #Trademark activation function for Logistic Regression (sigmoid/logistic function)\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    #Method to predict list of points of same number of fetures as those in training data\n",
    "    def predict(self,x_data, hard_threshold = 0.5):\n",
    "        #Hard threshold is in range[0,1] and decides if class labelled as 0 or 1. If HT is higher then more points will be\n",
    "        #classed as 1 and vice versa for 0. Best kept at 0.5 but can be varied if desired.\n",
    "        \n",
    "        y_hat = [self.sigmoid(np.dot(self.w,x) + self.b) for x in x_data] #probability point is of class 1\n",
    "\n",
    "        self.prediction = [int(y >= hard_threshold) for y in y_hat] #compare to HT, if greator or equal then label class 0, else class 1\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2  Train a logistic regressor using your code from Part 1, and see how it \n",
    "performs on both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1616025420500,
     "user": {
      "displayName": "Reggie Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggnd3P6tdKw4jehzlA9vIGoDnCp2KJPBxWEWWmn1Q=s64",
      "userId": "00203112220589711987"
     },
     "user_tz": 0
    },
    "id": "XprdRnCP3X5y",
    "outputId": "061d3bce-e226-4822-f05e-30a890c8c1c3"
   },
   "outputs": [],
   "source": [
    "#Also using function for my case studies assignment\n",
    "#data must be in form of pandas data frame, specify probabilities of train, validation, test\n",
    "def train_valid_test(data,p_train,p_validation,p_test):\n",
    "    num_data_points = len(data) #Store number of points\n",
    "    #Get number of validation and test points to use from probabilities give.\n",
    "    number_of_validation_points = int(num_data_points*p_validation) \n",
    "    number_of_test_points = int(num_data_points*p_test)\n",
    "    \n",
    "    #list of random indexes to sample from data for validation set, sorted to select these rows from data later\n",
    "    rows_for_validation = sorted(rand.sample(range(num_data_points), number_of_validation_points))\n",
    "\n",
    "    rows_for_test = []\n",
    "    #Populate rows for test incrementally from those not selected for validation\n",
    "    while len(rows_for_test) < number_of_test_points: \n",
    "        rand_number = rand.sample(range(num_data_points), 1)[0] # 1 random number in range [0,number data points-1]\n",
    "        if rand_number not in rows_for_validation + rows_for_test: #check if number is in validation selection\n",
    "            rows_for_test.append(rand_number) # if not then add to rows for test\n",
    "\n",
    "    rows_for_test = sorted(rows_for_test) #Sort this to subset data by the indexes in this list\n",
    "\n",
    "    #by elimination, add the remaining rows to train set \n",
    "    rows_for_train = [i for i in range(num_data_points) if i not in sorted(rows_for_validation + rows_for_test)]\n",
    "\n",
    "    #subset data into respective train, validation, test\n",
    "    train_data = data.loc[rows_for_train]\n",
    "    validation_data = data.loc[rows_for_validation]\n",
    "    test_data = data.loc[rows_for_test]\n",
    "    \n",
    "    return [train_data,validation_data,test_data]\n",
    "\n",
    "    \n",
    "#read datasets\n",
    "blobs = pd.read_csv(\"blobs250.csv\")\n",
    "moons = pd.read_csv(\"moons400.csv\")\n",
    "\n",
    "#Normalise the predictor variables in the datasets\n",
    "blobs[\"X0\"] = (blobs[\"X0\"]-blobs[\"X0\"].mean()) /blobs[\"X0\"].std()\n",
    "blobs[\"X1\"] = (blobs[\"X1\"]-blobs[\"X1\"].mean()) /blobs[\"X1\"].std()\n",
    "blobs[\"X2\"] = (blobs[\"X2\"]-blobs[\"X2\"].mean()) /blobs[\"X2\"].std()\n",
    "\n",
    "moons[\"X0\"] = (moons[\"X0\"]-moons[\"X0\"].mean()) /moons[\"X0\"].std()\n",
    "moons[\"X1\"] = (moons[\"X1\"]-moons[\"X1\"].mean()) /moons[\"X1\"].std()\n",
    "\n",
    "#Split as per specification\n",
    "blobs_train,blobs_validation,blobs_test = train_valid_test(blobs,0.7,0.15,0.15)\n",
    "moons_train,moons_validation,moons_test = train_valid_test(moons,0.7,0.15,0.15)\n",
    "\n",
    "\n",
    "#Split each data set into feature set and target set and normalise feature set\n",
    "#Split data into feature set and target set\n",
    "blobs_y_train = blobs_train[\"Class\"].to_numpy()\n",
    "blobs_X_train = blobs_train.drop(columns= \"Class\").to_numpy()\n",
    "\n",
    "blobs_y_validate = blobs_validation[\"Class\"].to_numpy()\n",
    "blobs_X_validate = blobs_validation.drop(columns= \"Class\").to_numpy()\n",
    "\n",
    "blobs_y_test = blobs_test[\"Class\"].to_numpy()\n",
    "blobs_X_test = blobs_test.drop(columns= \"Class\").to_numpy()\n",
    "\n",
    "\n",
    "moons_y_train = moons[\"Class\"].to_numpy()\n",
    "moons_X_train = moons.drop(columns= \"Class\").to_numpy()\n",
    "\n",
    "moons_y_validate = moons_validation[\"Class\"].to_numpy()\n",
    "moons_X_validate = moons_validation.drop(columns= \"Class\").to_numpy()\n",
    "\n",
    "moons_y_test = moons_test[\"Class\"].to_numpy()\n",
    "moons_X_test = moons_test.drop(columns= \"Class\").to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1616025420500,
     "user": {
      "displayName": "Reggie Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggnd3P6tdKw4jehzlA9vIGoDnCp2KJPBxWEWWmn1Q=s64",
      "userId": "00203112220589711987"
     },
     "user_tz": 0
    },
    "id": "XprdRnCP3X5y",
    "outputId": "061d3bce-e226-4822-f05e-30a890c8c1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs accuracy before tuning = 1.0 : learning rate = 0.1\n",
      "moons accuracy before tuning = 0.905 : learning rate = 0.1\n",
      "blobs accuracy after tuning = 1.0 : learning rate = 0.15\n",
      "moons accuracy after tuning = 0.903 : learning rate = 0.1\n"
     ]
    }
   ],
   "source": [
    "#Time to test\n",
    "\n",
    "#First test performance using default parameters learning_rate = 0.1 ,max_iter = 10000 ,threshold = 1e-6\n",
    "#testing on test data, but saving validation set for tuning\n",
    "learning_rate = 0.1 \n",
    "max_iter = 10000\n",
    "threshold = 1e-6\n",
    "\n",
    "my_logistic = Logistic()\n",
    "blobs_score = []\n",
    "moons_score = []\n",
    "for i in range(50):    #taking an average of 50 predictions\n",
    "    rand.seed(seed+i)\n",
    "    #Blobs\n",
    "    my_logistic.fit(blobs_X_train,blobs_y_train,learning_rate,max_iter,threshold)\n",
    "    predicted = my_logistic.predict(blobs_X_test)\n",
    "    blobs_score.append(np.mean(blobs_y_test == predicted)) #caclculates accuracy of predictions\n",
    "\n",
    "  #Moons\n",
    "    my_logistic.fit(moons_X_train,moons_y_train,learning_rate,max_iter,threshold)\n",
    "    predicted = my_logistic.predict(moons_X_test)\n",
    "    moons_score.append(np.mean(moons_y_test == predicted))     \n",
    "        \n",
    "print(\"blobs accuracy before tuning = \" + str(np.round(np.mean(blobs_score),3)) +' : learning rate = 0.1')\n",
    "print(\"moons accuracy before tuning = \" + str(np.round(np.mean(moons_score),3)) +' : learning rate = 0.1')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Tune\n",
    "learning_rates = [0.15,0.125,0.1,0.075,0.05,0.01,0.001] #range of learning rates to test, noting that 0.1 perfomed well originally\n",
    "#Keeping these constant and focusing on finding optimal learning rate\n",
    "max_iter = 10000\n",
    "threshold = 1e-6\n",
    "\n",
    "blobs_training_score = []\n",
    "moons_training_score = []\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    predicted_moons = []\n",
    "    predicted_blobs = []\n",
    "    for i in range(15):    #lower averaging to save computation time\n",
    "        rand.seed(seed+i)\n",
    "        #Blobs\n",
    "        my_logistic.fit(blobs_X_train,blobs_y_train,learning_rate ,max_iter ,threshold )\n",
    "        predicted_blobs.append(my_logistic.predict(blobs_X_validate))\n",
    "      #Moons\n",
    "        my_logistic.fit(moons_X_train,moons_y_train,learning_rate ,max_iter ,threshold)\n",
    "        predicted_moons.append(my_logistic.predict(moons_X_validate))\n",
    "        \n",
    "    blobs_training_score.append([learning_rate,np.mean(blobs_y_validate == predicted_blobs)])\n",
    "    moons_training_score.append([learning_rate,np.mean(moons_y_validate == predicted_moons)])\n",
    "#save index of learning rate from score... \n",
    "index_of_best_blobs_LR = np.argmax([blobs_training_score[i][1] for i in range(len(learning_rates))])\n",
    "index_of_best_moons_LR = np.argmax([moons_training_score[i][1] for i in range(len(learning_rates))])\n",
    "\n",
    "#Specify best learning rates for each dataset\n",
    "best_blobs_LR = learning_rates[index_of_best_blobs_LR]\n",
    "best_moons_LR = learning_rates[index_of_best_moons_LR]\n",
    "\n",
    "my_logistic = Logistic()\n",
    "\n",
    "#Test again with tuned parameters\n",
    "blobs_score = []\n",
    "moons_score = []\n",
    "for i in range(50):    \n",
    "    rand.seed(seed+i)\n",
    "    #Blobs\n",
    "    my_logistic.fit(blobs_X_train,blobs_y_train,best_blobs_LR ,max_iter ,threshold )\n",
    "    predicted = my_logistic.predict(blobs_X_test)\n",
    "    blobs_score.append(np.mean(blobs_y_test == predicted))\n",
    "\n",
    "  #Moons\n",
    "    my_logistic.fit(moons_X_train,moons_y_train,best_moons_LR,max_iter ,threshold)\n",
    "    predicted = my_logistic.predict(moons_X_test)\n",
    "    moons_score.append(np.mean(moons_y_test == predicted))     \n",
    "        \n",
    "print(\"blobs accuracy after tuning = \" + str(np.round(np.mean(blobs_score),3)) +' : learning rate = ' + str(best_blobs_LR ))\n",
    "print(\"moons accuracy after tuning = \" + str(np.round(np.mean(moons_score),3)) +' : learning rate = ' + str(best_moons_LR ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning,I found that the learning rate stayed the same for the moons dataset, making no major difference to the accuracucy , however for moons the learning rate was changed, but the accuracy remained constant. After 50 iterations averaged, the blobs dataset was labelled correctly 100% of the time for a learning rate of 0.1 and the moons data set labelled correctly 90% of the time for a learning rate of 0.1 . Moons is indeed the dataset which is not linearly sepeable and thus would generally be harder to predict than the blobs dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Implement and Test a Shallow Neural Network\n",
    "\n",
    "The class Neural Net takes inputs ; pred_vars,target_var,learning_rate,max_iter and threshold, same as logistic regression.\n",
    "Extra inputs include , epochs : number of forward and backward passes of all data points in training set\n",
    "int batch_size : how many points to pass through the forward and back propagation at once\n",
    "float converge_thresh : The threshold at which point the difference in cross entropy loss between epochs converges to.\n",
    "str Activation_fn: Speicifies which activation function to use, takes values 'relu','leaky_relu','tanh','logistic', default 'logistic'.\n",
    "list shape: takes form [a,b,c...] where each a,b,c.. is an integer which represents number of nodes in hidden layer. The length of this represents how many hidden layers, e.g. [10] represents a NN with 1 hidden layer with 10 nodes in it, [10,10,2] a NN with 3 hidden layers of nodes, 10 , 10 and 2 each layer. A logistic regression would be represented as [] which is a 0 hidden layer neural net.\n",
    "int output: number of output nodes in the final leyer\n",
    "float regularisation_penalty : penalty term to reduce complexity in the network\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1616025420500,
     "user": {
      "displayName": "Reggie Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggnd3P6tdKw4jehzlA9vIGoDnCp2KJPBxWEWWmn1Q=s64",
      "userId": "00203112220589711987"
     },
     "user_tz": 0
    },
    "id": "XprdRnCP3X5y",
    "outputId": "061d3bce-e226-4822-f05e-30a890c8c1c3"
   },
   "outputs": [],
   "source": [
    "class Neural_net:\n",
    "    \n",
    "    class node: #node class resembling that of logistic class as above, repurposed for Neural Net\n",
    "\n",
    "        def __init__(self,num_vars,batch_size,activation_fn = 'logistic',regularisation = 0):\n",
    "            self.num_vars = num_vars # num variables fed into node from previous layer\n",
    "            self.batch_size = batch_size #overall batch size of NN\n",
    "            self.activation_fn = activation_fn  #specified activation function\n",
    "            self.regularisation = regularisation\n",
    "\n",
    "        def randomise_w_b(self): # initialise random weights and bias for node with float value in range [0,1]\n",
    "            self.w =[rand.random() for i in range(self.num_vars)]\n",
    "            self.b =rand.random()\n",
    "\n",
    "        def update_w_b(self,learning_rate): # update w and b after each epoch\n",
    "            self.w -= np.multiply(learning_rate,self.delta_w)\n",
    "            self.b -= learning_rate*self.delta_b\n",
    "\n",
    "        def update_a(self,val): #method to feed input data points of length batch_size into nodes layer 0\n",
    "            self.a = val\n",
    "\n",
    "\n",
    "        def forward(self, layer_before): #forward propagation, taking input from nodes in the layer before the current one\n",
    "            self.z = [np.sum([np.dot(self.w,[batch[j] for batch in layer_before]) + self.b]) for j in range(self.batch_size)]\n",
    "            self.a = [self.f(z) for z in self.z]\n",
    "    \n",
    "  \n",
    "        def start_backward(self,y,layer_before): #run this method on each node on the output layer\n",
    "            #Formulae as per the notes, with added consideration for mini batch\n",
    "            self.delta_z = [self.a[i] - y[i] for i in range(self.batch_size)]\n",
    "            self.delta_w = [np.multiply(self.delta_z[ind],[node.a[ind] for node in layer_before]) \n",
    "                            for ind in range(self.batch_size)] + np.multiply(self.regularisation/self.batch_size,self.w)\n",
    "                #The addition term in delta w takes the L2 regularisation parameter into consideration, note default\n",
    "                #value is 0 so this does not affect the calculation unless the user wishes\n",
    "            \n",
    "            #take average of all values in batch to update w and b later\n",
    "            self.delta_w = [np.mean([self.delta_w[i][j] for i in range(self.batch_size)]) for j in range(self.num_vars)]\n",
    "            self.delta_b = np.mean(self.delta_z)\n",
    "\n",
    "\n",
    "        def backward(self,layer_before,layer_after): # Run this method for each in all other layers\n",
    "            #equations as per the notes, similar setup the start_backwards method\n",
    "            self.delta_z = np.multiply([self.f_prime(z) for z in self.z],\n",
    "                                 [np.sum([np.multiply(node.delta_z[ind],node.w) for node in layer_after]) for ind in range(self.batch_size)]) \n",
    "            \n",
    "            self.delta_w = [np.multiply(self.delta_z[ind],[node.a[ind] for node in layer_before])\n",
    "                            for ind in range(self.batch_size)] + np.multiply(self.regularisation/self.batch_size,self.w)\n",
    "\n",
    "            self.delta_w = [np.mean([self.delta_w[i][j] for i in range(self.batch_size)]) for j in range(self.num_vars)]\n",
    "            self.delta_b = np.mean(self.delta_z)\n",
    "\n",
    "            \n",
    "        def start_prediction(self,inp): # similar to the update a method as above.\n",
    "            #feeds the x values to be predicted into the nodes in layer 0 of the network\n",
    "            self.prediction = inp\n",
    "\n",
    "        def predict(self,x,return_probs=False, hard_threshold = 0.5):\n",
    "            #passes the x values to be predicted through the network to return a binary value if return probs = false\n",
    "            #else return probability of class 1\n",
    "            y_hat = self.f(np.dot(self.w,x) + self.b)\n",
    "            if  return_probs == False:\n",
    "                self.prediction = int(y_hat >= hard_threshold)\n",
    "            else:\n",
    "                self.prediction = y_hat\n",
    "            return self.prediction\n",
    "        \n",
    "        def f(self,z): #activation function which checks which fn to use\n",
    "            if self.activation_fn == 'relu':\n",
    "                f_z = self.ReLU(z)\n",
    "            elif self.activation_fn == 'leaky_relu':\n",
    "                f_z = self.leakyReLU(z)\n",
    "            elif self.activation_fn == 'tanh':\n",
    "                f_z = self.tanh(z)\n",
    "            else:\n",
    "                f_z = self.sigmoid(z)\n",
    "            \n",
    "            return f_z\n",
    "        \n",
    "        def f_prime(self,z): #derivative of activation function\n",
    "            if self.activation_fn == 'relu':\n",
    "                fprime_z = self.ReLU_prime(z)\n",
    "            elif self.activation_fn == 'leaky_relu':\n",
    "                fprime_z = self.leakyReLU_prime(z)\n",
    "            elif self.activation_fn == 'tanh':\n",
    "                fprime_z = self.tanh_prime(z)\n",
    "            else:\n",
    "                fprime_z = self.sigmoid_prime(z)\n",
    "            \n",
    "            return fprime_z        \n",
    "        \n",
    "        #Different functions as found in the notes\n",
    "        def sigmoid(self,z):\n",
    "            return 1/(1+np.exp(-z))\n",
    "        \n",
    "        def sigmoid_prime(self,z):\n",
    "            return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "        \n",
    "        def tanh(self,z):\n",
    "            return (np.exp(z)-np.exp(-z))/(np.exp(z)+ np.exp(-z))\n",
    "        \n",
    "        def tanh_prime(self,z):\n",
    "            return 1 - self.tanh(z)**2\n",
    "        \n",
    "        def ReLU(self,z):\n",
    "            return 0 if z<0.5 else z\n",
    "    \n",
    "        def ReLU_prime(self,z):\n",
    "            return 0 if z<0.5 else 1\n",
    "        \n",
    "        def leakyReLU(self,z):\n",
    "            return 0.01*z if z<0.5 else z\n",
    "    \n",
    "        def leakyReLU_prime(self,z):\n",
    "            return 0.01 if z<0.5 else 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, pred_vars,target_var,learning_rate = 0.1 ,max_iter = 10000,converge_thresh = 0.01 ,activation_fn = 'logistic',\n",
    "            epochs = 5,batch_size = 32,shape = [],output = 1,regularisation_penalty = 0):\n",
    "        #providing default values for all parameters, except the data\n",
    "        #similar setup to logistic regression\n",
    "        self.x = pred_vars\n",
    "        self.y = target_var \n",
    "        self.num_vars = len(pred_vars[0])\n",
    "        self.num_points = len(target_var)\n",
    "        self.output = output # save number of outputs desired\n",
    "    \n",
    "        #The shape of the NN is built from , layer 0, which has number of nodes equal to number of input features,\n",
    "        #hidden layers as specified by user, and the output layer populated with whatever the user specifies.\n",
    "        shape =  [self.num_vars] + shape + [output]\n",
    "\n",
    "        self.length = len(shape) # length of Neural net (input + #hidden + output layers)\n",
    "        \n",
    "        #input vars represents how many nodes from the previous layer are used to feed forward into the current layer,\n",
    "        #layer 0 has 0 nodes feeding into them, hence the 0. Then append the shape list as a layer 1 node would have all\n",
    "        #layer 0's nodes feeding into it.\n",
    "        input_vars = [0] + shape\n",
    "        \n",
    "        #Build the neural net as specified, placing a node class at each place a node is expected\n",
    "        self.NN = [[self.node(num_vars = input_vars[j],batch_size = batch_size,activation_fn = activation_fn,\n",
    "                              regularisation = regularisation_penalty) for i in range(shape[j])] for j in range(self.length) ]\n",
    "        \n",
    "        [node.randomise_w_b() for layer in self.NN for node in layer ] #initiate weights and biases for each node in NN\n",
    "        iters = 0  #initialising the iteration number\n",
    "        current_epoch = 1 #Starting 1 as I use >= in the while statement. If epochs is = 5 then \n",
    "                          #there will be 5 epochs of the NN. Begining at 1 means the systems prints the correct\n",
    "                          #current epoch as below \n",
    "        indexes = [i for i in range(self.num_points)] #List of indexes of the training points. This will be sampled from\n",
    "                                # without replacement, then once it empties, one epoch finishes and it is replenished again\n",
    "        prev_cross_ent_loss = 100 #initialise a high value for loss\n",
    "        while iters < max_iter and epochs >= current_epoch: # stop learning if max iterations or max epochs reached\n",
    "            if len(indexes) < batch_size: #if list of indexes is less than the points required for current batch\n",
    "                if current_epoch == epochs:\n",
    "                  #print(\"Finished Training\")  #uncomment out if desired\n",
    "                  break\n",
    "                #print(\"epoch # \" + str(current_epoch) +\" complete\")  #uncomment out if desired\n",
    "                sample = indexes #sample the remaining indexes \n",
    "                to_keep = sample # as these points are from this epochs, we dont wish to exclude them from the next epoch\n",
    "                indexes = [i for i in range(self.num_points)] #replenish the index list\n",
    "                current_epoch += 1 #next epoch officially started\n",
    "                \n",
    "                s_i = self.predict(x_i,return_probs=True) #test learned model performance so far on a set of size batch size\n",
    "                cross_ent_loss = -np.sum(np.multiply(y_i,np.log(s_i)))/batch_size #calculate the corss entropy loss\n",
    "                if prev_cross_ent_loss - cross_ent_loss <converge_thresh : #check if system has learned enough\n",
    "                    #if the system has achieved the convergence threshold, or has got worse, stop learning.\n",
    "                    #print(\"convergence reached after \" + str(current_epoch) + \" epochs\")\n",
    "                    break\n",
    "                prev_cross_ent_loss = cross_ent_loss # update loss\n",
    "                \n",
    "                while len(sample) < batch_size: #now to top up sample until it has length batch size\n",
    "                    to_add = rand.sample(indexes,batch_size- len(sample)) # sample numbers from indexes of length required\n",
    "                                                                        # to top up sample to length batch size\n",
    "                    [sample.append(num) for num in to_add if num not in sample] #check if the sampled number is already in sample,\n",
    "                                                                            #if not append to list, if so resample from indexes\n",
    "                to_remove = [sample[i] for i in range(batch_size) if sample[i] not in to_keep]\n",
    "                #points we wish to remove from this epoch as they are now being sampled from this current epoch\n",
    "                \n",
    "            else: #if length of indexes is big enough for the batch size\n",
    "                sample = rand.sample(indexes,batch_size) #take a sample without replacement\n",
    "                to_remove = sample #points to remove from the indexes list\n",
    "        \n",
    "            indexes = [indexes[i] for i in range(len(indexes)) if indexes[i] not in to_remove] #remove the sampled points from \n",
    "                                            #indexes so the system doesn't re select them in this epoch\n",
    "\n",
    "            x_i = [self.x[rand_num] for rand_num in sample] #choose random batch as per the list of sampled indexes\n",
    "            y_i = [self.y[rand_num] for rand_num in sample]\n",
    "            \n",
    "            #feed input points into nodes in layer 0\n",
    "            [self.NN[0][ind].update_a([x_i[b][ind] for b in range(batch_size)]) for ind in range(self.num_vars)]\n",
    "            #propagate forward through the system \n",
    "            [[node.forward([prev_node.a for prev_node in self.NN[i-1]]) for node in self.NN[i]]  for i in range(1,self.length) ]\n",
    "            iters += 1 # another pass through the system complete\n",
    "            \n",
    "            #Begin back propagation with all nodes in final layer\n",
    "            [self.NN[-1][i].start_backward(y_i,self.NN[-2]) for i in range(output)]\n",
    "\n",
    "      \n",
    "            for layer_ind in range((self.length-2),0,-1): #traverse backwards through net not including last layer\n",
    "                next_layer = self.NN[layer_ind+1] #taking the layer either side of the current one\n",
    "                layer_before = self.NN[layer_ind-1]\n",
    "                for node in self.NN[layer_ind]: #backpropagate for each node in the current layer\n",
    "                    node.backward(layer_before,next_layer)\n",
    "            \n",
    "            iters += 1 # another pass through the system complete\n",
    "            \n",
    "            #update weights and biases using learning rate specified\n",
    "            [node.update_w_b(learning_rate) for layer in self.NN[1:len(self.NN)] for node in layer]\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,x_data,return_probs = False):\n",
    "        #takes a list of data points of same number of features as taken by the fit method\n",
    "        #Can specify to return category labels, or probability of label\n",
    "        prediction = []\n",
    "    \n",
    "        for point in x_data: #iterate through each point given to be predicted\n",
    "            [self.NN[0][i].start_prediction(point[i]) for i in range(len(point))] #feed input points into nodes in layer 0\n",
    "            [[node.predict([prev_node.prediction for prev_node in self.NN[i-1]],return_probs=return_probs) for node in self.NN[i]]  for i in range(1,len(self.NN))]\n",
    "            #feed the points forward through the network and output a prediction\n",
    "            prediction.append([self.NN[-1][i].prediction for i in range(self.output)]) #append the prediction made in every output node\n",
    "\n",
    "        return prediction #return the list of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1616025425826,
     "user": {
      "displayName": "Reggie Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggnd3P6tdKw4jehzlA9vIGoDnCp2KJPBxWEWWmn1Q=s64",
      "userId": "00203112220589711987"
     },
     "user_tz": 0
    },
    "id": "4OhF7Sv_eaZq",
    "outputId": "4678b94f-4f25-41ec-a68a-f4a42c65c2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs accuracy for Neural net immitating a logistic regression = 0.582 : learning rate = 0.1\n",
      "moons accuracy for Neural net immitating a logistic regression = 0.9 : learning rate = 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NN = Neural_net()\n",
    "#Testing the NN with 0 hidden layers,the default logistic activation function and a batch size of one to emulate a logistic regression\n",
    "#using stochastic gradient descent\n",
    "blobs_score = []\n",
    "moons_score = []\n",
    "learning_rate = 0.1\n",
    "max_iter = 10000\n",
    "epochs = 20\n",
    "for i in range(50):    #taking an average of 50 predictions\n",
    "    rand.seed(seed+i) #change the random seed at each iteration to expose learning to different scenarios but still maintain reproducibility\n",
    "    #Blobs\n",
    "    NN.fit(blobs_X_train,blobs_y_train,learning_rate ,max_iter ,epochs = epochs,batch_size = 1,shape = [] )\n",
    "    predicted = NN.predict(blobs_X_test)\n",
    "    blobs_score.append(np.mean(blobs_y_test == predicted))\n",
    "  \n",
    "    NN.fit(moons_X_train,moons_y_train,learning_rate,max_iter ,epochs = epochs,batch_size = 1,shape = [] )\n",
    "    predicted = my_logistic.predict(moons_X_test)\n",
    "    moons_score.append(np.mean(moons_y_test == predicted))     \n",
    "  \n",
    "        \n",
    "print(\"blobs accuracy for Neural net immitating a logistic regression = \" + str(np.round(np.mean(blobs_score),3)) +' : learning rate = 0.1')\n",
    "print(\"moons accuracy for Neural net immitating a logistic regression = \" + str(np.round(np.mean(moons_score),3)) +' : learning rate = 0.1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this should behave like a logistic regression, the results are radically different. The big difference which gives this result is that the basic logistic regression evaluates its performance at every iteration, however the neural net structure evaluates at every epoch, and it also uses a cross entropy loss function which differs from that implemented for the logistic regression.\n",
    "The results are a slight improvement for the non-linearly seperable moons dataset, however a radically worse prediction score for the linearly seperable blobs dataset. My theory is that the NN is overfitting the blobs data as it learns for many more iterations than the logistic regression before being evaluated given a certain threshold. The moons dataset accuracy has improved as it requires a more complicated model to fit it, which the NN achieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24765,
     "status": "ok",
     "timestamp": 1616025456349,
     "user": {
      "displayName": "Reggie Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggnd3P6tdKw4jehzlA9vIGoDnCp2KJPBxWEWWmn1Q=s64",
      "userId": "00203112220589711987"
     },
     "user_tz": 0
    },
    "id": "80NhRU-vvObw",
    "outputId": "d43a3e78-240c-44ac-e401-2f0d71f394f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs training scores\n",
      "   accuracy  num_nodes\n",
      "0  0.517896         10\n",
      "1  0.517896         25\n",
      "2  0.517896         50\n",
      "3  0.517896         75\n",
      "4  0.517896        100\n",
      "5  0.517896        200\n",
      "moons training scores\n",
      "   accuracy  num_nodes\n",
      "0  0.501778         10\n",
      "1  0.521667         25\n",
      "2  0.526667         50\n",
      "3  0.526667         75\n",
      "4  0.527333        100\n",
      "5  0.505222        200\n",
      "blobs accuracy for Neural net 1 hidden layer and 10 nodes = 0.582\n",
      "moons accuracy for Neural net with 1 hidden layer and 100 nodes = 0.9\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_net()\n",
    "\n",
    "#Now train the neural net with with hidden layer with varying number of nodes in hidden layer\n",
    "#Using a sigmoid activation function and keeping everything else constant\n",
    "\n",
    "max_iter = 10000\n",
    "learning_rate = 0.1 \n",
    "max_iter = 10000 \n",
    "activation_fn = 'logistic'\n",
    "epochs = 20\n",
    "shapes = [[10],[25],[50],[75],[100],[200]] #various different shapes to try\n",
    "\n",
    "\n",
    "blobs_training_score = []\n",
    "moons_training_score = []\n",
    "for shape in shapes: # loop through all shapes to test\n",
    "    blobs_score = []\n",
    "    moons_score = []\n",
    "    for i in range(10): #average over 10 iterations of the one shape\n",
    "        rand.seed(seed+i)\n",
    "        #Blobs\n",
    "        NN.fit(blobs_X_train,blobs_y_train,learning_rate,max_iter,epochs = epochs,batch_size = 1,shape = shape)\n",
    "        predicted = NN.predict(blobs_X_validate)\n",
    "        actual = blobs_y_validate\n",
    "        blobs_score.append(np.mean(actual == predicted))\n",
    "        \n",
    "        #Moons\n",
    "        NN.fit(moons_X_train,blobs_y_train,learning_rate,max_iter,epochs = epochs,batch_size = 1,shape = shape)\n",
    "        predicted = NN.predict(moons_X_validate)\n",
    "        actual = moons_y_validate\n",
    "        moons_score.append(np.mean(actual == predicted))\n",
    "    blobs_training_score.append([np.mean(blobs_score),shape[0]])  #average the accuracy of 10 iterations and save the\n",
    "    moons_training_score.append([np.mean(moons_score),shape[0]])  #num nodes used \n",
    "    \n",
    "#save index of highest score to extract the number of nodes used\n",
    "index_of_best_blobs_numnodes = np.argmax([blobs_training_score[i][0] for i in range(len(shapes))])\n",
    "index_of_best_moons_numnodes = np.argmax([moons_training_score[i][0] for i in range(len(shapes))])\n",
    "\n",
    "#Specify best shape for each dataset\n",
    "best_blobs_shape = shapes[index_of_best_blobs_numnodes]\n",
    "best_moons_shape = shapes[index_of_best_moons_numnodes]\n",
    "\n",
    "#Store data in a pandas dataframe to visualise\n",
    "blobs_training = pd.DataFrame(blobs_training_score,columns = [\"accuracy\",\"num_nodes\"]) \n",
    "moons_training = pd.DataFrame(moons_training_score,columns = [\"accuracy\",\"num_nodes\"]) \n",
    "\n",
    "print(\"blobs training scores\")\n",
    "print(blobs_training)\n",
    "print(\"moons training scores\")\n",
    "print(moons_training)\n",
    "\n",
    "blobs_score = []\n",
    "moons_score = []\n",
    "#using best shape from training, test on test set\n",
    "for i in range(50):    #taking an average of 50 predictions\n",
    "    rand.seed(seed+i)\n",
    "    #Blobs\n",
    "    NN.fit(blobs_X_train,blobs_y_train,learning_rate ,max_iter ,epochs = epochs,batch_size = 1,shape = best_blobs_shape )\n",
    "    predicted = NN.predict(blobs_X_test)\n",
    "    blobs_score.append(np.mean(blobs_y_test == predicted))\n",
    "  \n",
    "    NN.fit(moons_X_train,moons_y_train,learning_rate,max_iter ,epochs = epochs,batch_size = 1,shape = best_moons_shape )\n",
    "    predicted = my_logistic.predict(moons_X_test)\n",
    "    moons_score.append(np.mean(moons_y_test == predicted))     \n",
    "  \n",
    "        \n",
    "print(\"blobs accuracy for Neural net 1 hidden layer and \"  +str(best_blobs_shape[0]) + \" nodes = \" + str(np.round(np.mean(blobs_score),3)))\n",
    "print(\"moons accuracy for Neural net with 1 hidden layer and \"+str(best_moons_shape[0]) + \" nodes = \"  + str(np.round(np.mean(moons_score),3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, no improvement was gained from including a hidden layer. I also note that the performance on the moons data set was much higher when tested on the holdout data over the tests performed on the validation set as the data was divided completely randomly.. The only reason I can see why this might have occured is because only 10 iterations were used during training and 50 were used for the testing, perhaps some randomly generated weights gave a better starting point than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-cd61f3152bcd>:165: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_ent_loss = -np.sum(np.multiply(y_i,np.log(s_i)))/batch_size #calculate the corss entropy loss\n",
      "<ipython-input-101-cd61f3152bcd>:165: RuntimeWarning: invalid value encountered in multiply\n",
      "  cross_ent_loss = -np.sum(np.multiply(y_i,np.log(s_i)))/batch_size #calculate the corss entropy loss\n",
      "<ipython-input-101-cd61f3152bcd>:166: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_cross_ent_loss - cross_ent_loss <converge_thresh : #check if system has learned enough\n",
      "<ipython-input-101-cd61f3152bcd>:31: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.delta_w = [np.multiply(self.delta_z[ind],[node.a[ind] for node in layer_before])\n",
      "C:\\Users\\lucas\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "<ipython-input-101-cd61f3152bcd>:32: RuntimeWarning: invalid value encountered in multiply\n",
      "  for ind in range(self.batch_size)] + np.multiply(self.regularisation/self.batch_size,self.w)\n",
      "<ipython-input-101-cd61f3152bcd>:31: RuntimeWarning: overflow encountered in multiply\n",
      "  self.delta_w = [np.multiply(self.delta_z[ind],[node.a[ind] for node in layer_before])\n",
      "<ipython-input-101-cd61f3152bcd>:44: RuntimeWarning: overflow encountered in multiply\n",
      "  [np.sum([np.multiply(node.delta_z[ind],node.w) for node in layer_after]) for ind in range(self.batch_size)])\n",
      "<ipython-input-101-cd61f3152bcd>:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.delta_z = np.multiply([self.f_prime(z) for z in self.z],\n",
      "<ipython-input-101-cd61f3152bcd>:47: RuntimeWarning: invalid value encountered in multiply\n",
      "  for ind in range(self.batch_size)] + np.multiply(self.regularisation/self.batch_size,self.w)\n",
      "<ipython-input-101-cd61f3152bcd>:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.z = [np.sum([np.dot(self.w,[batch[j] for batch in layer_before]) + self.b]) for j in range(self.batch_size)]\n",
      "<ipython-input-101-cd61f3152bcd>:165: RuntimeWarning: invalid value encountered in log\n",
      "  cross_ent_loss = -np.sum(np.multiply(y_i,np.log(s_i)))/batch_size #calculate the corss entropy loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs training scores\n",
      "   accuracy activation_function\n",
      "0  0.547042                relu\n",
      "1  0.481081          leaky_relu\n",
      "2  0.594595                tanh\n",
      "3  0.517896            logistic\n",
      "moons training scores\n",
      "   accuracy activation_function\n",
      "0  0.533333                relu\n",
      "1  0.533333          leaky_relu\n",
      "2  0.475444                tanh\n",
      "3  0.520000            logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-cd61f3152bcd>:165: RuntimeWarning: invalid value encountered in log\n",
      "  cross_ent_loss = -np.sum(np.multiply(y_i,np.log(s_i)))/batch_size #calculate the corss entropy loss\n",
      "<ipython-input-101-cd61f3152bcd>:165: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_ent_loss = -np.sum(np.multiply(y_i,np.log(s_i)))/batch_size #calculate the corss entropy loss\n",
      "<ipython-input-101-cd61f3152bcd>:165: RuntimeWarning: invalid value encountered in multiply\n",
      "  cross_ent_loss = -np.sum(np.multiply(y_i,np.log(s_i)))/batch_size #calculate the corss entropy loss\n",
      "<ipython-input-101-cd61f3152bcd>:166: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_cross_ent_loss - cross_ent_loss <converge_thresh : #check if system has learned enough\n",
      "<ipython-input-101-cd61f3152bcd>:31: RuntimeWarning: overflow encountered in multiply\n",
      "  self.delta_w = [np.multiply(self.delta_z[ind],[node.a[ind] for node in layer_before])\n",
      "<ipython-input-101-cd61f3152bcd>:44: RuntimeWarning: overflow encountered in multiply\n",
      "  [np.sum([np.multiply(node.delta_z[ind],node.w) for node in layer_after]) for ind in range(self.batch_size)])\n",
      "<ipython-input-101-cd61f3152bcd>:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.delta_z = np.multiply([self.f_prime(z) for z in self.z],\n",
      "<ipython-input-101-cd61f3152bcd>:32: RuntimeWarning: invalid value encountered in multiply\n",
      "  for ind in range(self.batch_size)] + np.multiply(self.regularisation/self.batch_size,self.w)\n",
      "<ipython-input-101-cd61f3152bcd>:47: RuntimeWarning: invalid value encountered in multiply\n",
      "  for ind in range(self.batch_size)] + np.multiply(self.regularisation/self.batch_size,self.w)\n",
      "<ipython-input-101-cd61f3152bcd>:31: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.delta_w = [np.multiply(self.delta_z[ind],[node.a[ind] for node in layer_before])\n",
      "C:\\Users\\lucas\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "<ipython-input-101-cd61f3152bcd>:46: RuntimeWarning: overflow encountered in multiply\n",
      "  self.delta_w = [np.multiply(self.delta_z[ind],[node.a[ind] for node in layer_before])\n",
      "<ipython-input-101-cd61f3152bcd>:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.z = [np.sum([np.dot(self.w,[batch[j] for batch in layer_before]) + self.b]) for j in range(self.batch_size)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs accuracy for Neural net 1 hidden layer, 10 nodes and function tanh = 0.303\n",
      "moons accuracy for Neural net with 1 hidden layer, 100 nodes and function relu = 0.9\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_net()\n",
    "\n",
    "#Out of curiosity I will try different acivation functions with the Neural Net, although\n",
    "#I don't expect the others to perform as well at the logistic activation function\n",
    "\n",
    "\n",
    "max_iter = 10000\n",
    "learning_rate = 0.1 \n",
    "max_iter = 10000 \n",
    "activation_fn = ['relu','leaky_relu','tanh','logistic']\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "blobs_training_score = []\n",
    "moons_training_score = []\n",
    "for activation in activation_fn: # loop through all shapes to test\n",
    "    blobs_score = []\n",
    "    moons_score = []\n",
    "    for i in range(10): #average over 10 iterations of the one shape\n",
    "        rand.seed(seed+i)\n",
    "        #Blobs\n",
    "        NN.fit(blobs_X_train,blobs_y_train,learning_rate,max_iter,epochs = epochs,batch_size = 1,shape = best_blobs_shape, activation_fn = activation)\n",
    "        predicted = NN.predict(blobs_X_validate)\n",
    "        actual = blobs_y_validate\n",
    "        blobs_score.append(np.mean(actual == predicted))\n",
    "        \n",
    "        #Moons\n",
    "        NN.fit(moons_X_train,blobs_y_train,learning_rate,max_iter,epochs = epochs,batch_size = 1,shape = best_moons_shape, activation_fn = activation)\n",
    "        predicted = NN.predict(moons_X_validate)\n",
    "        actual = moons_y_validate\n",
    "        moons_score.append(np.mean(actual == predicted))\n",
    "    blobs_training_score.append([np.mean(blobs_score),activation])  #average the accuracy of 10 iterations and save the\n",
    "    moons_training_score.append([np.mean(moons_score),activation])  #activation function used \n",
    "    \n",
    "#save index of highest score to extract the number of nodes used\n",
    "index_of_best_blobs_function = np.argmax([blobs_training_score[i][0] for i in range(len(activation_fn))])\n",
    "index_of_best_moons_function = np.argmax([moons_training_score[i][0] for i in range(len(activation_fn))])\n",
    "\n",
    "#Specify best shape for each dataset\n",
    "best_blobs_function = activation_fn[index_of_best_blobs_function]\n",
    "best_moons_function = activation_fn[index_of_best_moons_function]\n",
    "\n",
    "#Store data in a pandas dataframe to visualise\n",
    "blobs_training = pd.DataFrame(blobs_training_score,columns = [\"accuracy\",\"activation_function\"]) \n",
    "moons_training = pd.DataFrame(moons_training_score,columns = [\"accuracy\",\"activation_function\"]) \n",
    "\n",
    "print(\"blobs training scores\")\n",
    "print(blobs_training)\n",
    "print(\"moons training scores\")\n",
    "print(moons_training)\n",
    "\n",
    "blobs_score = []\n",
    "moons_score = []\n",
    "#using best shape from training, test on test set\n",
    "for i in range(50):    #taking an average of 50 predictions\n",
    "    rand.seed(seed+i)\n",
    "    #Blobs\n",
    "    NN.fit(blobs_X_train,blobs_y_train,learning_rate ,max_iter ,epochs = epochs,batch_size = 1,shape = best_blobs_shape ,activation_fn = best_blobs_function )\n",
    "    predicted = NN.predict(blobs_X_test)\n",
    "    blobs_score.append(np.mean(blobs_y_test == predicted))\n",
    "  \n",
    "    NN.fit(moons_X_train,moons_y_train,learning_rate,max_iter ,epochs = epochs,batch_size = 1,shape = best_moons_shape, activation_fn = best_moons_function )\n",
    "    predicted = my_logistic.predict(moons_X_test)\n",
    "    moons_score.append(np.mean(moons_y_test == predicted))     \n",
    "  \n",
    "        \n",
    "print(\"blobs accuracy for Neural net 1 hidden layer, \"  +str(best_blobs_shape[0]) + \" nodes and function \"+ best_blobs_function +\" = \" + str(np.round(np.mean(blobs_score),3)))\n",
    "print(\"moons accuracy for Neural net with 1 hidden layer, \"+str(best_moons_shape[0]) + \" nodes and function \" + best_moons_function + \" = \"  + str(np.round(np.mean(moons_score),3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors indicate more consideration needs to be taken for using different activation functions with the cross entropy loss function, typically a softmax layer comes before the cross entropy loss calculation, however as the prediction values are binary I chose to just include the probability as output by the activation functions and not worry about splitting into two seperate probabilities for the softmax layer.\n",
    "Through tuning, the blobs validation set yielded best results when using a tanh function yet when applied to the test set performed horribly with 30% accuracy, this would be expected as a tan function assumes the data not linear, which blobs is (linearly seperable that is). Classic case of overfitting\n",
    "For the moons dataset, relu, leaky relu and logistic all performed much the same on the validation dataset, and when using a relu function on the test dataset yielded the same accuracy score 90% as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Challenging Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "error",
     "timestamp": 1616026293032,
     "user": {
      "displayName": "Reggie Murphy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggnd3P6tdKw4jehzlA9vIGoDnCp2KJPBxWEWWmn1Q=s64",
      "userId": "00203112220589711987"
     },
     "user_tz": 0
    },
    "id": "iyt1gh-BtXlr",
    "outputId": "9fb4248e-62ed-4350-91ce-da5a45cd4ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the batch is 4\n",
      "All keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "size of data in this batch: 10000 , size of labels: 10000\n",
      "<class 'numpy.ndarray'>\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "#Load Cifar dataset :  entire chunk taken from LoadCIFAR10.ipynb supplied by Dr. Michael Madden\n",
    "\n",
    "# This function taken from the CIFAR website\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "#   data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. \n",
    "#           The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "#           The image is stored in row-major order, so that the first 32 entries of the array are the red channel values \n",
    "#           of the first row of the image.\n",
    "#   labels -- a list of 10000 numbers in the range 0-9. \n",
    "#             The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualise(data, index):\n",
    "    # MM Jan 2019: Given a CIFAR data nparray and the index of an image, display the image.\n",
    "    # Note that the images will be quite fuzzy looking, because they are low res (32x32).\n",
    "\n",
    "    picture = data[index]\n",
    "    # Initially, the data is a 1D array of 3072 pixels; reshape it to a 3D array of 3x32x32 pixels\n",
    "    # Note: after reshaping like this, you could select one colour channel or average them.\n",
    "    picture.shape = (3,32,32) \n",
    "    \n",
    "    # Plot.imshow requires the RGB to be the third dimension, not the first, so need to rearrange\n",
    "    picture = picture.transpose([1, 2, 0])\n",
    "    plt.imshow(picture)\n",
    "    plt.show()\n",
    "\n",
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())\n",
    "\n",
    "\n",
    "data = batch1[b'data']\n",
    "labels = batch1[b'labels']\n",
    "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
    "print (type(data))\n",
    "print(data.shape)\n",
    "\n",
    "names = loadlabelnames()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "QvlFyw4jlPb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifar accuracy with 1 hidden layer and 50 nodes = 0.4794520547945205\n"
     ]
    }
   ],
   "source": [
    "#index for RGB colours respectively\n",
    "red = [i for i in range(1024)]\n",
    "green = [i + 1024 for i in range(1024)]\n",
    "blue = [i + 2048 for i in range(1024)]\n",
    "\n",
    "#Segment data into only one colour and normalising features in range [0,1] by dividing by the max pixel value 255\n",
    "normalised_red_data = [picture[red]/255 for picture in data]\n",
    "normalised_green_data = [picture[green]/255 for picture in data]\n",
    "normalised_blue_data = [picture[blue]/255 for picture in data]\n",
    "\n",
    "data = normalised_red_data #selecting red channel to test on\n",
    "\n",
    "classes = [b'dog',b'cat'] #Given classes to differentiate between\n",
    "\n",
    "#subset total dataset into just the two classes as above and label 0 if dog and 1 if cat\n",
    "labelled_subset = [[data[i],0 if names[labels[i]] == b'dog' else 1]  for i in range(len(data)) if names[labels[i]] in classes]\n",
    "\n",
    "#create a dataframe from this subset\n",
    "cifar = pd.DataFrame(labelled_subset,columns = [\"data\",\"Class\"]) \n",
    "\n",
    "\n",
    "#split the data by percentage\n",
    "cifar_train,cifar_validation,cifar_test = train_valid_test(cifar,0.7,0.15,0.15)\n",
    "\n",
    "\n",
    "cifar_y_train = cifar_train[\"Class\"].to_numpy()\n",
    "cifar_X_train = cifar_train['data'].to_numpy()\n",
    "\n",
    "cifar_y_validate = cifar_validation[\"Class\"].to_numpy()\n",
    "cifar_X_validate = cifar_validation[\"data\"].to_numpy()\n",
    "\n",
    "cifar_y_test = cifar_test[\"Class\"].to_numpy()\n",
    "cifar_X_test = cifar_test[\"data\"].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "NN = Neural_net()\n",
    "cifar_score = []\n",
    "for i in range(2):\n",
    "    rand.seed(seed+i)\n",
    "    NN.fit(cifar_X_train,cifar_y_train,learning_rate = 0.1,max_iter = 5000 ,epochs = epochs,batch_size = 1,shape = [50])\n",
    "    predicted = NN.predict(cifar_X_test)\n",
    "  \n",
    "    actual = cifar_y_test\n",
    "    cifar_score.append(np.mean(actual == predicted))\n",
    "\n",
    "\n",
    "print(\"Cifar accuracy with 1 hidden layer and 50 nodes = \" + str(np.mean(cifar_score) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48% accuracy for differentiating between cats and dogs. Due to the number of features I had to reduce the number of tests to average over in order to let the system complete in a manageable time frame. Taking this into consideration, I will not tune the parameters as my sysem could not handle that computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: Deep Learning Enhancements\n",
    "\n",
    "The enhancements which my Neural net supports are as follows:\n",
    "1. Any number of hidden layers and customizable nodes per layer, including output layer\n",
    "2. Mini batch gradient descent\n",
    "3. L2 Regularization\n",
    "4. Supports 4 different acitvation functions (minor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifar accuracy with 2 hidden layers,50 nodes each, batch size of 32, learning rate and regularisation penaly both equal to 0.1 = 0.5205479452054794\n"
     ]
    }
   ],
   "source": [
    "#Experimenting with common values for each hyperparameter\n",
    "LR = 0.1\n",
    "function = 'logistic'\n",
    "batch = 32  #batch size must be small realtive to dataset size, total dataset size is 2000, therefore 32 seems appropriate\n",
    "shape = [50,50] #including a second hidden layer\n",
    "penalty = 0.1 #including a L2 regularisation penalty value\n",
    "epochs = 20\n",
    "NN = Neural_net()\n",
    "cifar_score = []\n",
    "for i in range(2):\n",
    "    rand.seed(seed+i)\n",
    "    NN.fit(cifar_X_train,cifar_y_train,learning_rate = LR ,max_iter = 5000 ,activation_fn = function,\n",
    "                   epochs = epochs,batch_size = batch,shape = shape,output = 1,regularisation_penalty = penalty)\n",
    "    predicted = NN.predict(cifar_X_test)\n",
    "  \n",
    "    actual = cifar_y_test\n",
    "    cifar_score.append(np.mean(actual == predicted))\n",
    "\n",
    "\n",
    "print(\"Cifar accuracy with 2 hidden layers,50 nodes each, batch size of 32, learning rate and regularisation penaly both equal to 0.1 = \" + str(np.mean(cifar_score) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the batch size from 1 to 32, including an extra hidden layer of 50 nodes and including an L2 regularisation penalty term , the accuracy of the prediction increased by 4%. Perhaps with further tuning, and a deeper network this score would increase further. I contemplate this in the next chunk, but don't actually run it for computational complexity reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell only on a gpu , will take a very long time, 10's of hours I estimate\n",
    "#It iterates through multiple common values of every hyperparameter  and saves the training results in a csv file\n",
    "#Of course this would possibly lead to overfitting on the validation set, however by saving all the results,\n",
    "#one could see the tradeoff in accuracy and complexity.\n",
    "#I tried running this overnight on google collab however, collab timed out before completion theredfore I leave \n",
    "#only as an idea.\n",
    "\n",
    "import csv \n",
    "NN = Neural_net()\n",
    "\n",
    "activation_functions = ['relu','leaky_relu','tanh','logistic']\n",
    "learning_rates = [0.15,0.1,0.05,0.01,0.001]\n",
    "regularisation_penalties = [10,1,0.1,0.001]\n",
    "batch_sizes = [16,32,64]\n",
    "\n",
    "num_hidden_layers = [1,2,3,4]\n",
    "num_nodes_per_layer = [10,25,50,75,100]\n",
    "\n",
    "shapes = []\n",
    "for hidden_layer in num_hidden_layers:\n",
    "    for nodes in num_nodes_per_layer:\n",
    "        shapes.append([nodes]*hidden_leys) #various shapes\n",
    "        \n",
    "tuning_score = []\n",
    "for function in activation_functions:\n",
    "    for LR in learning_rates:\n",
    "        for penalty in regularisation_penalties:\n",
    "            for batch in batch_sizes:\n",
    "                for shape in shapes:\n",
    "                    \n",
    "                NN.fit(cifar_X_train,cifar_y_train,learning_rate = LR ,max_iter = 10000 ,activation_fn = function,\n",
    "                   epochs = 20,batch_size = batch,shape = shape,output = 1,regularisation_penalty = penalty)\n",
    "                prediction = NN.predict(cifar_X_validate)\n",
    "                accuracy = np.mean(prediction,cifar_y_validate)\n",
    "                tuning_Score.append([accuracy,function,LR,penalty,batch,shape])\n",
    "                \n",
    "with open('tuning_data.csv', 'w') as f: \n",
    "      \n",
    "    # save data as csv\n",
    "    write = csv.writer(f) \n",
    "      \n",
    "    write.writerows(tuning_score) \n",
    "    \n",
    "index_of_best_highest_accuarcy = np.argmax([moons_training_score[i][0] for i in range(len(tuning_Score))])   \n",
    "\n",
    "#extract values to use for testing\n",
    "score,function,Learning_rate,regularisation_penalty,batch_size,shape = tuning_Score[index_of_best_highest_accuarcy]\n",
    "\n",
    "test_score = []\n",
    "for i in range(20):\n",
    "    NN.fit(cifar_X_train,cifar_y_train,learning_rate = Learning_rate ,max_iter = 10000 ,activation_fn = function,\n",
    "                   epochs = 50,batch_size = batch_size,shape = shape,output = 1,regularisation_penalty = regularisation_penalty)\n",
    "    predicted = my_logistic.predict(moons_X_test)\n",
    "    test_score.append(np.mean(moons_y_test == predicted)) \n",
    "\n",
    "print('cifar accuracy after tuning = ' + str(np.mean(test_score)))\n",
    "print(\"activation function = \" +str(function) + \"; Learning_rate = \" \n",
    "      + str(learning_rate) + \"; penalty = \" +str(regularisation_penalty) + \"; batch size = \" \n",
    "      + str(batch_size) + \"; shape = \" + str(shape) )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNdlwQnClx7VGwSmx8FcWRt",
   "collapsed_sections": [],
   "name": "Deep_Learning_Assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
